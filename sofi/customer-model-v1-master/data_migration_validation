# TODO:
# write something that automates data validation
# do in own workspace
# from cinfig file with queries pull data, then descirbe describe_df


src_base = os.path.dirname(os.path.realpath(__file__))

config_file_path = os.path.abspath(os.path.join(src_base, "../config.json"))

config_file_path_postgres = os.path.abspath(os.path.join(src_base, "../config_postgres.json"))


with open(config_file_path, "r") as f:
    CONFIG_FILE = json.load(f)


with open(config_file_path_postgres, "r") as f:
    CONFIG_FILE_POSTGRES = json.load(f)


def get_snowflake_connection():
    ctx = snowflake.connector.connect(
        user="<username>",
        password="<password>",
        host="localhost",
        port=1444,
        account="sdm",
        warehouse="DATA_SCIENCE",
        database="DW_PRODUCTION",
        protocol="http",
    )
    return ctx


def run_query(sql):
    with get_snowflake_connection() as ctx:
        with ctx.cursor() as cs:
            cs.execute(sql)
            allthedata = cs.fetch_pandas_all()
            return allthedata


def query_data(base_path="data", prefix=None):
    """
    Get raw data with one or more queries.
    """
    timestamp_str = str(int(time.time()))

    for name, qinf in CONFIG_FILE["sql_query_files"].items():
        with open(os.path.join(src_base, qinf["query"])) as f:
            query = f.read()

        print("Running query {}.".format(name))
        df = run_query(query=query)
        save_dataframes(
            {name: df}, base_path, prefix, timestamp_str, include_subdir=True
        )

        # clear memory when running several queries
        del df
        gc.collect()

    _set_config_file_field("data_pull_date", pd.datetime.today().strftime("%Y-%m-%d"))
