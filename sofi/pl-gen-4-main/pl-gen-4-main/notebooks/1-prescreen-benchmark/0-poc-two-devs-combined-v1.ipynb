{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27bdfbc",
   "metadata": {},
   "source": [
    "### Combined V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebcbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, os, ast\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from smart_open import open\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "sys.path.insert(1, \"../..\")\n",
    "from src.logger import make_logger\n",
    "from src.dataloader import TabularDataloader\n",
    "from src.Trainer import LGBMTrainer, TFTrainer\n",
    "from src.preprocess import Preprocess\n",
    "\n",
    "from rdsutils.feature_selection import mrmr\n",
    "from rdsutils.woe import WOE_Transform\n",
    "from _utils.feature_selection import feature_selection as fs\n",
    "from _utils.performance_eval import performance_eval_v3 as p_eval\n",
    "from rdsutils.feature_selection import FeatureSelector as general_purpose_fsel\n",
    "from src.feature_selection import FeatureSelector  # to be moved to rdsutils\n",
    "\n",
    "# new modules\n",
    "from _utils.sample_weights import get_sample_weight\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd38e2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'meta', 'data_columns', 'model_params', 'model_features', 'impute_vals', 'monotone'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = 'target_v1'\n",
    "target_indeterminate = 'indeterminate_v1'\n",
    "weight = \"weight\"\n",
    "seed = 42\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "display(config.keys()) \n",
    "\n",
    "gen3_features = config[\"data_columns\"][\"gen3_features\"]\n",
    "gen3_params = config[\"model_params\"][\"gen3_params\"]\n",
    "if \"scale_pos_weight\" in gen3_params:\n",
    "    del gen3_params[\"scale_pos_weight\"]\n",
    "\n",
    "bureau_fts = config[\"data_columns\"][\"bureau_features_cols\"] \n",
    "cat_fts = ['t11_t3d_segid', 't11_t3d_segid_supp'] # config[\"data_columns\"][\"cat_cols\"] \n",
    "prescreen_fts = bureau_fts + cat_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bbe40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['all_features_dev1', 'all_features_dev2', 'all_features_oot1', 'all_features_oot2', 'subset_dev1', 'subset_dev2'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(config[\"data\"][\"clean\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe626095",
   "metadata": {},
   "source": [
    "#### load data and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c22e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228188, 5131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 5131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(215815, 5131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 5131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 3min 21s, total: 4min 31s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data dict\n",
    "exp_dict = pd.read_csv(config[\"meta\"][\"exp_dict_path\"])\n",
    "\n",
    "# fsel data - sampled\n",
    "dl1 = TabularDataloader(train_path=config[\"data\"][\"clean\"][\"subset_dev1\"])\n",
    "dl1.load_data(debug_size=10000, random_state=seed)\n",
    "\n",
    "debug_df1, _, _ = dl1.get_data(debug=True)\n",
    "train_df1, _, _ = dl1.get_data(debug=False)\n",
    "display(train_df1.shape, debug_df1.shape)\n",
    "\n",
    "dl2 = TabularDataloader(train_path=config[\"data\"][\"clean\"][\"subset_dev2\"])\n",
    "dl2.load_data(debug_size=10000, random_state=seed)\n",
    "\n",
    "debug_df2, _, _ = dl2.get_data(debug=True)\n",
    "train_df2, _, _ = dl2.get_data(debug=False)\n",
    "display(train_df2.shape, debug_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e29e15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444003, 5131)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df1, train_df2], axis=0)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5628e5e",
   "metadata": {},
   "source": [
    "#### get sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e519fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4205/4205 [00:13<00:00, 321.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        added columns:\n",
      "            weight: training sample_weight scaled using provided weights by ri_source\n",
      "                weight_eval * weight_sample\n",
      "        \n",
      "dropping indeterminate col: indeterminate_v1\n"
     ]
    }
   ],
   "source": [
    "col = \"ri_source\"\n",
    "weights = {\"booked\": 1,\n",
    "           \"proxy\": 1,\n",
    "           \"others\": 0.25}\n",
    "\n",
    "assert sorted(train_df[col].unique().tolist()) == sorted(list(weights.keys()))\n",
    "\n",
    "pp = Preprocess(exp_dict)\n",
    "train_df[\"weight_eval\"] = train_df[\"weight_cob\"] * train_df[\"weight_ri_v1\"]\n",
    "train_df = pp.transform(train_df, prescreen_fts, weights, \n",
    "                        drop_indeterminate=target_indeterminate, \n",
    "                        existing_weights_col=\"weight_eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41fac81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ri_source\n",
       "booked    26162.500\n",
       "others    44596.625\n",
       "proxy     38190.500\n",
       "Name: weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ri_source\n",
       "booked     26162.5\n",
       "others    178386.5\n",
       "proxy      38190.5\n",
       "Name: weight_eval, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at weights\n",
    "display(train_df[[\"weight\", \"ri_source\"]].groupby(\"ri_source\")[\"weight\"].sum())\n",
    "display(train_df[[\"weight_eval\", \"ri_source\"]].groupby(\"ri_source\")[\"weight_eval\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c54a9f",
   "metadata": {},
   "source": [
    "#### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d594adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f5600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_col: target_v1\n",
      "weight_col: weight\n",
      "Preprocessing... generating iv and shaps\n",
      "prepping woe...\n",
      "\u001b[32mAttrs removed--missing pct>99%:  \u001b[0m ['p13_all8162', 'p13_all8163', 'p13_all8380', 'p13_all8723', 'p13_all9222', 'p13_all9223', 'p13_all9230', 'p13_all9239', 'p13_all9240', 'p13_all9249', 'p13_all9260', 'p13_all9280', 'p13_aua8162', 'p13_aua8163', 'p13_bca0401', 'p13_bca5021', 'p13_bca6201', 'p13_col8194', 'p13_hlc5021', 'p13_iln0403', 'p13_mtf8169', 'p13_mtf8656', 'p13_mts8151', 'p13_rpm5020', 'p13_rpm5320', 'p13_rpm5820', 'p13_rpm6160', 'p13_rpm7110', 'p13_rti5020', 'p13_rti5320', 'p13_rti5820', 'p13_uti5030', 'p13_uti5530', 'p13_uti8151', 't11_tall1412', 't11_tall1413', 't11_tall2412', 't11_tcol2556', 't11_tcol2567', 't11_tmti0451', 't11_tmti0452', 't11_tmti0453', 't11_tmti0454', 't11_tmti0455', 't11_tmti0456', 't11_tmti0457', 't11_tmti0458', 't11_tstu0909']\n",
      "processed  4157  num attributes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping lgbm shap\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping lgbm mc shap\n",
      "p13_all9130 1 no monotonic direction - probably should filter out\n",
      "p13_all9134 1 no monotonic direction - probably should filter out\n",
      "p13_all9135 1 no monotonic direction - probably should filter out\n",
      "p13_all9138 1 no monotonic direction - probably should filter out\n",
      "p13_all9139 1 no monotonic direction - probably should filter out\n",
      "p13_all9140 1 no monotonic direction - probably should filter out\n",
      "p13_all9141 1 no monotonic direction - probably should filter out\n",
      "p13_all9144 1 no monotonic direction - probably should filter out\n",
      "p13_all9145 1 no monotonic direction - probably should filter out\n",
      "p13_all9148 1 no monotonic direction - probably should filter out\n",
      "p13_all9149 1 no monotonic direction - probably should filter out\n",
      "p13_all9171 1 no monotonic direction - probably should filter out\n",
      "p13_all9177 1 no monotonic direction - probably should filter out\n",
      "p13_all9178 1 no monotonic direction - probably should filter out\n",
      "p13_all9180 1 no monotonic direction - probably should filter out\n",
      "p13_all9187 1 no monotonic direction - probably should filter out\n",
      "p13_all9188 1 no monotonic direction - probably should filter out\n",
      "p13_all9189 1 no monotonic direction - probably should filter out\n",
      "p13_all9330 1 no monotonic direction - probably should filter out\n",
      "p13_all9340 1 no monotonic direction - probably should filter out\n",
      "p13_all9380 1 no monotonic direction - probably should filter out\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering features by logic - experian\n",
      "dropping 530 features : kept 3675 features\n",
      "    reason:  not AA\n",
      "160 features with greater than                 0.95 missing values\n",
      "dropping 160 features : kept 3515 features\n",
      "    reason:  too many missing\n",
      "dropping 569 features : kept 2946 features\n",
      "    reason:  low_iv\n",
      "running many to few\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [20:42<00:00,  6.21s/it]\n",
      "100%|██████████| 200/200 [24:07<00:00,  7.24s/it]\n",
      "100%|██████████| 200/200 [47:27<00:00, 14.24s/it]\n",
      "100%|██████████| 7/7 [00:01<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving ranking.csv\n",
      "running fsel on few\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "p13_all9130 1 no monotonic direction - probably should filter out\n",
      "p13_all9134 1 no monotonic direction - probably should filter out\n",
      "p13_all9135 1 no monotonic direction - probably should filter out\n",
      "p13_all9138 1 no monotonic direction - probably should filter out\n",
      "p13_all9139 1 no monotonic direction - probably should filter out\n",
      "p13_all9140 1 no monotonic direction - probably should filter out\n",
      "p13_all9141 1 no monotonic direction - probably should filter out\n",
      "p13_all9144 1 no monotonic direction - probably should filter out\n",
      "p13_all9145 1 no monotonic direction - probably should filter out\n",
      "p13_all9148 1 no monotonic direction - probably should filter out\n",
      "p13_all9149 1 no monotonic direction - probably should filter out\n",
      "p13_all9171 1 no monotonic direction - probably should filter out\n",
      "p13_all9177 1 no monotonic direction - probably should filter out\n",
      "p13_all9178 1 no monotonic direction - probably should filter out\n",
      "p13_all9180 1 no monotonic direction - probably should filter out\n",
      "p13_all9187 1 no monotonic direction - probably should filter out\n",
      "p13_all9188 1 no monotonic direction - probably should filter out\n",
      "p13_all9189 1 no monotonic direction - probably should filter out\n",
      "p13_all9330 1 no monotonic direction - probably should filter out\n",
      "p13_all9340 1 no monotonic direction - probably should filter out\n",
      "p13_all9380 1 no monotonic direction - probably should filter out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving ranking.csv\n",
      "CPU times: user 3h 30min 59s, sys: 52min 8s, total: 4h 23min 8s\n",
      "Wall time: 2h 25min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%capture record\n",
    "\n",
    "nr_to_consider = 200\n",
    "nr_to_select = 200\n",
    "fsel_dir = \"./artifacts/dev_combined_fsel_v1\"\n",
    "\n",
    "fsel = FeatureSelector(train_df, data_dict=exp_dict)\n",
    "rankings = fsel.run(prescreen_fts, target, weight, nr_to_consider, nr_to_select,\n",
    "                    output_dir=fsel_dir, filter_by_logic_expn=True)\n",
    "\n",
    "# with open(\"./artifacts/dev1_fsel_1/log.txt\", \"w\") as f:\n",
    "#     f.write(record.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef81caa",
   "metadata": {},
   "source": [
    "##### get fsel results\n",
    "\n",
    "* run `fsel.get_rankings(True)` to get ranking_df of features that is ever selected.\n",
    "\n",
    "\n",
    "##### if computation already made, we can just load it\n",
    "\n",
    "```python\n",
    "# initialte project and load back\n",
    "fsel = FeatureSelector(train_df, data_dict=exp_dict)\n",
    "fsel.load_state_dict(fsel_dir)\n",
    "```\n",
    "\n",
    "##### this is the logic underneath fsel.run\n",
    "\n",
    "```python\n",
    "# setup\n",
    "features = prescreen_fts\n",
    "target_col = target\n",
    "weight_col = weight\n",
    "output_dir = fsel_dir\n",
    "corr_threshold = 0.8\n",
    "filter_by_logic_expn = True\n",
    "\n",
    "# first preprocessing\n",
    "fsel.preprocess(features, target_col, weight_col, output_dir=output_dir)\n",
    "\n",
    "if filter_by_logic_expn:\n",
    "    print(\"filtering features by logic - experian\")\n",
    "    features = fsel.filter_by_logic_expn(features, target_col, weight_col)\n",
    "\n",
    "fsel.many_to_few(features, target_col, weight_col, nr_to_consider)\n",
    "if output_dir: fsel.save_state_dict(output_dir)\n",
    "\n",
    "# get top <nr_to_select> features by mean just as a rule of a thumb\n",
    "rankings_imp = fsel.get_rankings(True)\n",
    "rankings_imp[\"<mean>\"] = rankings_imp.mean(axis=1)\n",
    "rankings_imp.sort_values(\"<mean>\", inplace=True)\n",
    "top_features = rankings_imp.index.to_list()\n",
    "rankings_imp.drop(\"<mean>\", axis=1, inplace=True)\n",
    "\n",
    "# to approximate number of features to consider so\n",
    "# we end up nr_to_select features when using the less efficient \n",
    "# methods\n",
    "\n",
    "approx_nr_to_select = int(nr_to_select / (corr_threshold+0.001))\n",
    "\n",
    "fsel.fsel_on_few(top_features[:approx_nr_to_select], target_col, \n",
    "                 weight_col, corr_threshold=corr_threshold)\n",
    "if output_dir: fsel.save_state_dict(output_dir)\n",
    "\n",
    "rankings = fsel.get_rankings(False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1060ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsel2 = FeatureSelector(train_df, data_dict=exp_dict)\n",
    "fsel2.load_state_dict(fsel_dir)\n",
    "fts = fsel2.get_rankings(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd4b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212802e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61675c13",
   "metadata": {},
   "source": [
    "#### build base model, set on features\n",
    "\n",
    "# issue: feature selector did not consider categorical variables.... since we only have < 5 of them, treat manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56da56",
   "metadata": {},
   "source": [
    "#### hyperparam tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75901995",
   "metadata": {},
   "source": [
    "#### model eval\n",
    "---\n",
    "* evaluation segments\n",
    "    * `weight`\n",
    "    * around score cut\n",
    "    * booked, proxy, others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86ed9d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2868, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings = pd.read_csv(\"./artifacts/dev1_fsel_v1/ranking.csv\", index_col=0)\n",
    "rankings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d3215f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n"
     ]
    }
   ],
   "source": [
    "# hand made shap\n",
    "\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "\n",
    "default_params = {\n",
    " 'objective': 'binary',\n",
    " 'metric': 'auc',\n",
    " 'boosting': 'gbdt',\n",
    " 'max_depth': 6,\n",
    " 'learning_rate': 0.05,\n",
    " 'min_data_in_leaf': [300],\n",
    " 'verbosity': -1,\n",
    " 'seed': 157,\n",
    " 'n_jobs': 30,\n",
    " 'n_estimators': 1000\n",
    "}\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(**default_params)\n",
    "\n",
    "list_features = rankings.index.to_list()\n",
    "trainer = LGBMTrainer()\n",
    "trainer.train(lgbm, \n",
    "              train_df,\n",
    "              features = list_features,\n",
    "              target_col = target,\n",
    "              sample_weight = train_df[weight]\n",
    "             )\n",
    "explainer = shap.TreeExplainer(lgbm)\n",
    "shap_values = explainer.shap_values(train_df[list_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84fe7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_features = pd.DataFrame(shap_values[1], columns=list(train_df[list_features]))\\\n",
    "                    .apply(lambda x: np.abs(x).mean(), axis=0)\\\n",
    "                    .sort_values(ascending=False)\n",
    "\n",
    "benchmark_fts = shap_features.index.to_list()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c2353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "features = benchmark_fts\n",
    "from rdsutils.feature_selection.WeightedCorr import WeightedCorr\n",
    "\n",
    "corr_matrix = WeightedCorr(df=train_df[benchmark_fts+[weight]], wcol=weight)(\"pearson\").abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "reduced_shap_features = [f for f in features if f not in (to_drop)]\n",
    "print(len(reduced_shap_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0646f9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_shap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc43b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./artifacts/models/model_params.json\", \"r\") as f:\n",
    "    model_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "364e2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p13_all9123 1\n",
      "p13_all9130 1\n",
      "p13_all9134 1\n",
      "p13_all9135 1\n",
      "p13_all9138 1\n",
      "p13_all9139 1\n",
      "p13_all9140 1\n",
      "p13_all9141 1\n",
      "p13_all9144 1\n",
      "p13_all9145 1\n",
      "p13_all9148 1\n",
      "p13_all9149 1\n",
      "p13_all9171 1\n",
      "p13_all9177 1\n",
      "p13_all9178 1\n",
      "p13_all9180 1\n",
      "p13_all9187 1\n",
      "p13_all9188 1\n",
      "p13_all9189 1\n",
      "p13_all9330 1\n",
      "p13_all9340 1\n",
      "p13_all9380 1\n"
     ]
    }
   ],
   "source": [
    "def get_monotone_dir(woe_dict):\n",
    "    result = {}\n",
    "    for k in woe_dict:\n",
    "        tbl = woe_dict[k]\n",
    "        if len(tbl) < 2:\n",
    "            print(k, len(tbl))\n",
    "        elif tbl.iloc[0][\"woe\"] < tbl.iloc[1][\"woe\"]:\n",
    "            direction = 1\n",
    "        else:\n",
    "            direction = -1\n",
    "        \n",
    "        result[k] = direction\n",
    "    return result\n",
    "\n",
    "with open(\"./artifacts/dev2_fsel_v1/woe_dict.pkl\", \"rb\") as f:\n",
    "    woe_dict = pkl.load(f)\n",
    "\n",
    "monotone_dict = get_monotone_dir(woe_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83bf7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_ = reduced_shap_features[:32]\n",
    "params_ = copy.deepcopy(model_params[\"dev1_v1_benchmark\"][\"params\"])\n",
    "\n",
    "fts_mc = fts_\n",
    "params_mc = copy.deepcopy(params_)\n",
    "mc = [monotone_dict[ft] for ft in fts_mc]\n",
    "params_mc[\"monotone_constraints\"] = mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae7d1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_v1_benchmark = {\"features\": fts_,\n",
    "                     \"params\": params_,\n",
    "                     \"model_type\": \"lightgbm\"}\n",
    "combined_v1_benchmark_mc = {\"features\": fts_mc,\n",
    "                     \"params\": params_mc,\n",
    "                     \"model_type\": \"lightgbm\"}\n",
    "model_params[\"combined_v1_benchmark\"] = combined_v1_benchmark\n",
    "model_params[\"combined_v1_benchmark_mc\"] = combined_v1_benchmark_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de948764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev1_v1_benchmark', 'dev1_v1_benchmark_mc', 'dev1_v1_fsel_32', 'dev1_v1_fsel_32_mc', 'dev2_v2_benchmark', 'dev2_v2_benchmark_mc', 'dev2_v1_benchmark', 'dev2_v1_benchmark_mc', 'combined_v1_benchmark', 'combined_v1_benchmark_mc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "695d50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./artifacts/models/model_params.json\", \"w\") as f:\n",
    "    json.dump(model_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2a8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pl_gen4",
   "language": "python",
   "name": "conda_pl_gen4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
