{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Notebook for Scorecard design\n",
    "---\n",
    "\n",
    "Goal: to develope a modularized, resuable scorecard module automation package, that would wore decently well without much tuning.\n",
    "\n",
    "A typical model consists the following states: \n",
    "* Model design\n",
    "* Model development\n",
    "* Model validation, recalibration, and monitoring\n",
    "\n",
    "In this task we will focus on model development stage, of which human intervention is minimized. With such principle in mind, the task can be broken down into the following bins:\n",
    "* Segmentation\n",
    "* Variable selection\n",
    "* Regression line\n",
    "* Model selection\n",
    "\n",
    "**Segmentation** consists the purpose of allowing different scorecards to be built for each subgroups of the population, with the purpose of achieving better perfomrance than a single scorecard could. One assumption is by manually providing segmentation, the scorecard would boost performances either by the aid of nonlinearity, regularization, or else. \n",
    "* Can be segmentated by different categorical attributes. This is essentially an ensemble approach. \n",
    "* consult the [BaggingClassifier](https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/ensemble/_bagging.py#L433) or any other meta-estimators like GridSearchCV, SelectFromModel\n",
    "* or build a [VotingClassier](https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/ensemble/_voting.py#L141) that takes in indicators of which component model to be used.\n",
    "    * for each individual scorecard, maybe have a method to output weight\n",
    "    * out voting classifier will normalize the weight from different models, and return result with soft/hard voting.\n",
    "\n",
    "**Variable selection** many ways works here, but the standard comes with selection by IV to systematically reduce from a huge list of variables. After a reasonable numbers of features left, we can safely use something like Boruta or Boruta Shap to pickout the rest. \n",
    "* A simple IV + Boruta Pipeline currently should do the trick\n",
    "* But may need to consider the option of doing feature selection pipeline within each segmented groups. Is there a clean way to do it? \n",
    "    * Since the segmentation we mentioned above should be manual/business driven. We can simply construct pipeline for each of them\n",
    "    * and combine with a Meta Classifier\n",
    "    \n",
    "**Regression line** At the bottom it is a simple linear regression, but to get it in shape the model requires a strong WOE transform algorithm to turn the features into monotonic bins. This should be addressed with the IV pipeline as suggested above since WOE is an intermediate solution from IV.\n",
    "\n",
    "**Model Selection** If we follow the manual segmentation ideology from above, the smart thing would be simply using the Random Search approach.\n",
    "\n",
    "### Summary\n",
    "We discussed a few ideas above. Here are the must-have modules to get it work:\n",
    "* WOE\n",
    "* IV\n",
    "* A MetaClassifier takes estimators, and criteria indicating a data point should be evaluated by any subset of available models. \n",
    "\n",
    "And lastly:\n",
    "* A Pipe example\n",
    "* Feature reduction approach to drop correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_basic_py37",
   "language": "python",
   "name": "ml_basic_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
