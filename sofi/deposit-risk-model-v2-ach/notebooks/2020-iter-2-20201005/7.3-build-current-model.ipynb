{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ACH model - iter2 - final\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task\n",
    "\n",
    "from rdsutils import datagen\n",
    "import rdsutils.plot as rdsplot\n",
    "from rdsutils.lightgbm_helpers import train_lgb_baseline_grouped\n",
    "\n",
    "sys.path.insert(1, '../../')\n",
    "from src.utils import preprocess\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = pd.read_parquet('../../artifacts/20201005/modeling_df_w_baseline_preds.parquet')\n",
    "test_df = pd.read_parquet('../../artifacts/20201005/test_df.parquet')\n",
    "modeling_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = preprocess(modeling_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    108825\n",
       "True       3774\n",
       "Name: is_returned, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling_df.is_returned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    84689\n",
       "True      4201\n",
       "Name: is_returned, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.is_returned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df['account_ending_balance'] = modeling_df['real_ending_balance']\n",
    "modeling_df['days_since_first_transaction'] = modeling_df['days_since_first_deposit']\n",
    "modeling_df['ach_target'] = modeling_df['is_returned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2 Model Fitting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pkl.load(open('../../artifacts/20201005/features_corr_removed.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "check_features = False\n",
    "if check_features:\n",
    "    for f in features:\n",
    "        try:\n",
    "            modeling_df[f].hist(bins=100)\n",
    "            plt.title(f)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../artifacts/20201005/final_lgbm_params.json', 'r') as f:\n",
    "    params_autogluon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "\n",
    "target_col = 'ach_target'\n",
    "count_pos = modeling_df[target_col].sum()\n",
    "count_neg = (~modeling_df[target_col]).sum()\n",
    "pos_wgt_scaling_factor = count_neg / count_pos\n",
    "\n",
    "params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting\": 'gbdt',\n",
    "    \"max_depth\" : 3,\n",
    "    \"num_leaves\" : 10,\n",
    "    \"learning_rate\" : 0.02,\n",
    "    \"feature_fraction\" : 0.6,\n",
    "    \"lambda_l1\": 10,\n",
    "    \"lambda_l2\": 10, \n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"scale_pos_weight\": pos_wgt_scaling_factor,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    \"bagging_seed\" : seed,\n",
    "    \"verbosity\" : -1,\n",
    "    \"seed\": seed\n",
    "}\n",
    "\n",
    "for p in params_autogluon:\n",
    "    params[p] = params_autogluon[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, fimp, models, auc = train_lgb_baseline_grouped(modeling_df, features, \n",
    "                                                     params, target_col, seed=seed)\n",
    "modeling_df['boruta_pred_1005_clean_features'] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdsplot.display_feature_importance(fimp.feature, \n",
    "                                   fimp.importance,\n",
    "                                   max_n_features=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df['boruta_pred_1005_clean_features'].hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Stats\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from rdsutils.plot import plot_auc_curve_mult, plot_pr_curve_mult, plot_feature_over_time\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "def get_binary_metrics(y_true, y_pred):\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    from scikitplot.helpers import binary_ks_curve\n",
    "    \n",
    "    auc = round(roc_auc_score(y_true=y_true,\n",
    "                              y_score=y_pred)*100, 2)\n",
    "    ap = round(average_precision_score(y_true=y_true,\n",
    "                                       y_score=y_pred)*100, 2)\n",
    "    _, _, _, ks, _, _ = binary_ks_curve(y_true=y_true, y_probas=y_pred)\n",
    "    ks = round(ks*100, 2) \n",
    "    \n",
    "    metrics = {'auc': auc,\n",
    "               'ap': ap,\n",
    "               'ks': ks}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_pred_reports(df, target_col, pred_cols):\n",
    "    import pandas as pd\n",
    "    result = {}\n",
    "    for col in pred_cols:\n",
    "        metrics = get_binary_metrics(df[target_col], df[col])\n",
    "        result[col] = metrics\n",
    "    return pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_pred_reports(modeling_df, target_col, \n",
    "                           ['deposit_v1_pred', \n",
    "                            'deposit_v1_updated_pred', \n",
    "                            'customer_pred',\n",
    "                            'boruta_pred_1005', \n",
    "                            'boruta_pred_1005_clean_features'])\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [(modeling_df['deposit_v1_pred'], 'deposit_v1_pred'), \n",
    "         (modeling_df['customer_pred'], 'customer_pred'),\n",
    "         (modeling_df['boruta_pred_1005_clean_features'], 'boruta_pred_1005_clean_features')]\n",
    "\n",
    "title = 'Precision-Recall curve: Baseline Comparison'\n",
    "plot_pr_curve_mult(modeling_df[target_col], preds,\n",
    "                   title=title, colors = ['r', 'g', 'b']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'AUC-ROC curve: Baseline Comparison'\n",
    "plot_auc_curve_mult(modeling_df[target_col], preds,\n",
    "                   title=title, colors = ['r', 'g', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with valid FICO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_fico = modeling_df[(~modeling_df.fico_score.isna()) & (modeling_df.fico_score <= 850)]\n",
    "# logically good fico score -> lower fraud prob\n",
    "# flip the direction to match with target \n",
    "\n",
    "df_w_fico['fico_pred'] = -df_w_fico['fico_score']  \n",
    "metrics = get_pred_reports(df_w_fico, target_col, \n",
    "                           ['fico_pred', \n",
    "                            'deposit_v1_pred', \n",
    "                            'customer_pred',\n",
    "                            'boruta_pred_1005_clean_features'])\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [(df_w_fico['fico_pred'], 'fico_pred'),\n",
    "         (df_w_fico['deposit_v1_pred'], 'deposit_v1_pred'), \n",
    "         (df_w_fico['customer_pred'], 'customer_pred'),\n",
    "         (df_w_fico['boruta_pred_1005_clean_features'], 'boruta_pred_1005_clean_features')]\n",
    "\n",
    "title = 'Precision-Recall curve: Baseline Comparison'\n",
    "plot_pr_curve_mult(df_w_fico[target_col], preds,\n",
    "                   title=title, colors = ['r', 'g', 'b', 'orange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'AUC-ROC curve: Baseline Comparison'\n",
    "plot_auc_curve_mult(df_w_fico[target_col], preds,\n",
    "                   title=title, colors = ['r', 'g', 'b', 'orange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study Performance on Segments\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_col = 'boruta_pred_1005_clean_features'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df[pred_col].hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdsplot.hist_by_target(pred_col, target_col, modeling_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def print_metric_by_time_bin(modeling_df, target_col, pred_col, metric_fn=roc_auc_score):\n",
    "#     dtmp = modeling_df[modeling_df.transaction_datetime < pd.to_datetime('2020-05-01')]\n",
    "#     print(\"before May 2020\", roc_auc_score(y_true=dtmp[target_col], y_score=dtmp[pred_col]))\n",
    "\n",
    "    dtmp = modeling_df[modeling_df.transaction_datetime.between(pd.to_datetime('2020-05-01'), pd.to_datetime('2020-05-31'))]\n",
    "    print(\"May 2020:\", metric_fn(y_true=dtmp[target_col], y_score=dtmp[pred_col]))\n",
    "    \n",
    "    dtmp = modeling_df[modeling_df.transaction_datetime.between(pd.to_datetime('2020-06-01'), pd.to_datetime('2020-06-30'))]\n",
    "    print(\"June 2020:\", metric_fn(y_true=dtmp[target_col], y_score=dtmp[pred_col]))\n",
    "    \n",
    "    dtmp = modeling_df[modeling_df.transaction_datetime.between(pd.to_datetime('2020-07-01'), pd.to_datetime('2020-07-31'))]\n",
    "    print(\"July 2020:\", metric_fn(y_true=dtmp[target_col], y_score=dtmp[pred_col]))\n",
    "    \n",
    "#     dtmp = modeling_df[modeling_df.transaction_datetime > pd.to_datetime('2019-04-14')] #invalid fico\n",
    "#     print(\"04/14/19-present:\", roc_auc_score(y_true=dtmp['target'], y_score=dtmp['pred']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- AUC ---')\n",
    "print_metric_by_time_bin(modeling_df, target_col, pred_col, roc_auc_score)\n",
    "\n",
    "print('--- AP ---')\n",
    "print_metric_by_time_bin(modeling_df, target_col, pred_col, average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df_older_accs = modeling_df[modeling_df.nr_past_transactions >= 10] \n",
    "modeling_df_younger_accs = modeling_df[modeling_df.nr_past_transactions < 10]\n",
    "\n",
    "print('Older account: ', roc_auc_score(y_true=modeling_df_older_accs[target_col], y_score=modeling_df_older_accs[pred_col]))\n",
    "print('Younger account: ', roc_auc_score(y_true=modeling_df_younger_accs[target_col], y_score=modeling_df_younger_accs[pred_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df_fico = modeling_df[~modeling_df.fico_score.isna()] \n",
    "modeling_df_no_fico = modeling_df[modeling_df.fico_score.isna()]\n",
    "\n",
    "print('w FICO account: ', roc_auc_score(y_true=modeling_df_fico[target_col], y_score=modeling_df_fico[pred_col]))\n",
    "print('no FICO account: ', roc_auc_score(y_true=modeling_df_no_fico[target_col], y_score=modeling_df_no_fico[pred_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway\n",
    "\n",
    "Challenges:\n",
    "- Older accounts\n",
    "- Accounts w/o credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ml_basic_py37",
   "language": "python",
   "name": "conda_ml_basic_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
