{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47976647",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "---\n",
    "\n",
    "A light-weight Dataset class object that can be used with:\n",
    "* saving and loading - holistic or by chunks\n",
    "* processing\n",
    "\n",
    "Future\n",
    "* meta-data management\n",
    "* automatic processing/clipping\n",
    "* validation\n",
    "* weighing\n",
    "* splitting\n",
    "\n",
    "\n",
    "Can study AIF360 dataset objects for inspiration\n",
    "* https://github.com/Trusted-AI/AIF360/blob/master/aif360/datasets/dataset.py\n",
    "* https://github.com/Trusted-AI/AIF360/blob/master/aif360/datasets/structured_dataset.py\n",
    "* https://github.com/Trusted-AI/AIF360/blob/746e763191ef46ba3ab5c601b96ce3f6dcb772fd/aif360/datasets/binary_label_dataset.py#L6\n",
    "\n",
    "The Dataset Object essentially have 3 stages\n",
    "* Load\n",
    "* Process\n",
    "* Save\n",
    "\n",
    "The exact methods should be able to be customized, but we should provide a basic framework for the pattern. My current thought is to have a generator producing paths used to load and save the data, and the provided processing function will be applied on each chunk.\n",
    "\n",
    "This process will be inplemented in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a390c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (5.0.0)\n",
      "Requirement already satisfied: pyarrow in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (4.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (4.60.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas smart_open pyarrow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b3c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, warnings, smart_open, shutil\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89f5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdsutils.datasets import Dataset, StructuredDataset, DataLoader, DataDumper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1141ed",
   "metadata": {},
   "source": [
    "#### Example Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c569cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.titanic.enums import features, target, cat_idx, cat_features, num_features, categorical_encoder\n",
    "\n",
    "train = pd.read_csv('../data/titanic/train.csv', index_col=0)\n",
    "valid = pd.read_csv('../data/titanic/valid.csv', index_col=0)\n",
    "test = pd.concat([train, valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341503e3",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f621f05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test (418, 11)\n",
      "train (623, 13)\n",
      "valid (268, 13)\n"
     ]
    }
   ],
   "source": [
    "# load data iteratively with a generator\n",
    "dl = DataLoader(\"../data/titanic\", suffix=\"csv\")\n",
    "for fname, df__ in dl:\n",
    "    print(fname, df__.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038b4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/projects-framework/rdsutils/examples/../data/titanic/test.csv',\n",
       " '/home/ec2-user/SageMaker/projects-framework/rdsutils/examples/../data/titanic/train.csv',\n",
       " '/home/ec2-user/SageMaker/projects-framework/rdsutils/examples/../data/titanic/valid.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all loaded file paths\n",
    "display(dl.get_paths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f48637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all files and concat\n",
    "df = dl.get_full()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c0962",
   "metadata": {},
   "source": [
    "#### Save\n",
    "Use pathlib\n",
    "* https://stackoverflow.com/questions/42407976/how-to-load-multiple-text-files-from-a-folder-into-a-python-list-variable\n",
    "\n",
    "What is needed for Dumper?\n",
    "* provide a dirctory path. create on if does not exist\n",
    "* when given a path, save it as parquet with the provided keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352601a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving test\n",
      "saving train\n",
      "saving valid\n"
     ]
    }
   ],
   "source": [
    "# how to create a pipeline: load and dump\n",
    "dl = DataLoader(\"../data/titanic\", suffix=\"csv\")\n",
    "dd = DataDumper(\"../data/titanic-copy\")\n",
    "\n",
    "for fname, df_ in dl:\n",
    "    print(f\"saving {fname}\")\n",
    "    dt_str = str(int(dt.datetime.now().timestamp()))\n",
    "    dd.to_parquet(df_, fname+\"_\"+dt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5e1aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/projects-framework/rdsutils/examples/../data/titanic-copy/test_1621275145.parquet',\n",
       " '/home/ec2-user/SageMaker/projects-framework/rdsutils/examples/../data/titanic-copy/train_1621275145.parquet',\n",
       " '/home/ec2-user/SageMaker/projects-framework/rdsutils/examples/../data/titanic-copy/valid_1621275145.parquet']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1621275145 (418, 11)\n",
      "train_1621275145 (623, 13)\n",
      "valid_1621275145 (268, 13)\n"
     ]
    }
   ],
   "source": [
    "# verify load and dumped are the same\n",
    "dl = DataLoader(\"../data/titanic-copy\", suffix=\"parquet\")\n",
    "display(dl.get_paths())\n",
    "for fname, df__ in dl:\n",
    "    print(fname, df__.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13bd5c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     True\n",
       "PassengerId    True\n",
       "Survived       True\n",
       "Pclass         True\n",
       "Name           True\n",
       "Sex            True\n",
       "Age            True\n",
       "SibSp          True\n",
       "Parch          True\n",
       "Ticket         True\n",
       "Fare           True\n",
       "Cabin          True\n",
       "Embarked       True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_ == df__) | (df_.isna() & df__.isna())).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732f9a1",
   "metadata": {},
   "source": [
    "#### Test dumping into multiple file functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7148bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/titanic/train.csv', index_col=0)\n",
    "valid = pd.read_csv('../data/titanic/valid.csv', index_col=0)\n",
    "test = pd.read_csv('../data/titanic/test.csv', index_col=0)\n",
    "\n",
    "train[\"type\"] = \"train\"\n",
    "valid[\"type\"] = \"valid\"\n",
    "test[\"type\"] = \"test\"\n",
    "df_full = pd.concat([train, valid, test])\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d8f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 337.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_full[df_full.type == t] for t in df_full.type.unique()]\n",
    "\n",
    "dd = DataDumper(\"../data/titanic-copy2\")\n",
    "dd.to_parquets(dfs, \"by_types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b186b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\"../data/titanic-copy2\", suffix=\"parquet\")\n",
    "df_full_ = dl.get_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e8f0165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    True\n",
       "Survived       True\n",
       "Pclass         True\n",
       "Name           True\n",
       "Sex            True\n",
       "Age            True\n",
       "SibSp          True\n",
       "Parch          True\n",
       "Ticket         True\n",
       "Fare           True\n",
       "Cabin          True\n",
       "Embarked       True\n",
       "type           True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the reconstructed data is equivalent to the loaded one\n",
    "df_ = df_full_.sort_index()\n",
    "df__ = df_full.sort_index()\n",
    "((df_ == df__) | (df_.isna() & df__.isna())).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da124631",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e8efe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"../data/titanic-copy/\", ignore_errors=True)\n",
    "shutil.rmtree(\"../data/titanic-copy2/\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a7d88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4de11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
