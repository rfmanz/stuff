{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='D:/Downloads/AI_Safety_and_The_Legacy_of_Bletchley_Park.mp3-extract.mp3'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pydub import AudioSegment\n",
    "\n",
    "# startMin = 2\n",
    "# startSec = 50\n",
    "\n",
    "# endMin = 3\n",
    "# endSec = 50\n",
    "\n",
    "# # Time to miliseconds\n",
    "# startTime = startMin*60*1000+startSec*1000\n",
    "# endTime = endMin*60*1000+endSec*1000\n",
    "\n",
    "# # Opening file and extracting segment\n",
    "# song = AudioSegment.from_mp3(podcast)\n",
    "# extract = song[startTime:endTime]\n",
    "\n",
    "# # Saving\n",
    "# extract.export(podcast+'-extract.mp3', format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast = \"D:/Downloads/AI_Safety_and_The_Legacy_of_Bletchley_Park.mp3\"\n",
    "podcast_snippet = \"D:/Downloads/AI_Safety_and_The_Legacy_of_Bletchley_Park.mp3-extract.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " intelligence in the context of human intelligence. I tend to disagree with the basic notion that intelligence is defined through things that humans can do. Intelligence, I think, is very difficult to define, but I don't think it is well defined by, it's what humans do. I think it's a more abstract notion than that. And a lot of the conversations that inevitably happen around AI and AI safety involve a definition of super intelligence that invariably corresponds to, well, when we can't think of anything that a human can do better. I view this as not really interestingly different from the situation now in which humans are good at some things and machines are good at other things. And gradually, machines get better at some of the things that humans are good at. That is to say that the set of things that we can solve with what we call typically weak AI tends to increase over time. So it used to be that, I don't know, computers couldn't play checkers. And now, computers can play checkers better than people. And that used to be true for chess.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(podcast_snippet)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Downloads/AI_Safety_and_The_Legacy_of_Bletchley_Park.mp3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Open an audio file and read as mono waveform, resampling as necessary\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    file: str\n",
      "        The audio file to open\n",
      "\n",
      "    sr: int\n",
      "        The sample rate to resample the audio if necessary\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    A NumPy array containing the audio waveform, in float32 dtype.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(whisper.load_audio.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
